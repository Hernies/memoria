\chapter{Fundamentos y Estado del Arte}
\label{ch:fundamentos}

\section{Conceptos generales}
\label{se:ConceptosGeneralesa}

El concepto de NILM desde su introducción ha sido el método preferido por ingenieros e investigadores para la disgregación de consumo eléctrico, debido a sus ventajas económicas y prácticas.\autocite[pág. 2, pár. 4]{Nalmpantis2019}

En cuanto al origen de este método, George Hart buscó categorizar en \autocite{192069} los dispositivos dentro de tres grandes grupos, para poder estructurar los diferentes dispositivos y generalizar el modelado de dispositivos específicos. 
Estos grupos son: 

\begin{enumerate}
\item \textbf{Señales de estados fijos} Siguen la arquitecturea de una máquina de estados finita, las transiciones entre estados son consumos exactos, al igual que los estados
\item \textbf{Señales transitorias} Esto son señales que no se encuentran en un espacio discreto que permita modelar su comportamiento como una máquina de estados. Un ejemplo podría ser un calefactor automático, que regula su generación de calor en función de la diferencia de la temperatura ambiente y la temperatura objetivo.
\item \textbf{Otras} Señales que no pueden categorizarse en ninguno de los dos casos anteriores. Por ejemplo, los cambios de dirección del motor de una lavadora en el proceso de lavado. 
\end{enumerate}

Es importante mencionar que los propios investigadores reconocieron que en el futuro los dispositivos eléctricos que siguiesen un diseño de máquinas de estados perderían precedencia. Ya que habría más 'dispositivos inteligentes' \autocite{192069}. Actualmente, una calefacción programable o un aire acondicionado programable están al alcance de cualquiera.
En España, en el transcurso de 2021 el 50\% de las viviendas en alquiler constaban de aire acondicionado\autocite{idealista2021}. 

Y esto es sólo un dispositivo. Habrá cientos de modelos de cada dispositivo, decenas de marcas, cada dispositivo con sus características específicas. 
Un modelado exhaustivo sería prácticamente imposible de crear y mantener, ya que sólo para el mercado doméstico hay una cantidad desorbitante de dispositivos. Modelar todas las posibles combinaciones no es una posibilidad real.

\subsubsection{Un ejemplo de sistema NILM}
Un sistema NILM consta de: Una fuente de datos (acometida principal). Un dispositivo de recogida de datos (puede ser de Potencia Activa Y reactiva, Voltaje e Intensidad, etc). El modelo predictivo. Y el sistema de consulta de datos, que se ha omitido en la Figura \ref{diagramaNILM}.
\begin{figure}
    \centering
    \includegraphics[width=450px]{images/SistemaNILM.pdf}
    \caption{Un diagrama simple de un sistema NILM \\ Fuente: Elaboración propia}
    \label{diagramaNILM}
\end{figure}

Debido a estos argumentos, la investigación de la monitorización no intrusiva se inclinó a los modelos heurísiticos y el aprendizaje automático. Debido a sus ventajas y prevalencia en el estado del arte (ver \ref{RN},\ref{SOTA})


\section{Aprendizaje Automático}

En la Historia de la Inteligencia Artificial, se han definido una serie de etapas de gran crecimiento y financiación como  \foreignquote{spanish}{veranos} \autocite{briefAIHistory} (tres concretamente). Alternados por (dos) \foreignquote{spanish}{inviernos}; donde el interés, la financiación y el número de publicaciones decrecían significativamente \autocite{briefAIHistory}.
En los principios de la década de los 2010, comenzó el tercer \enquote{verano} de la Inteligencia Artificial. En 2012 ocurre un avance técnico: unos años antes, Olga Russakovsky creó el reto ImageNet \autocite{ImageNetRussakovsky}; un repositorio de más de un millón de imágenes de objetos a lo largo de más de mil categorías diferentes. 
Este reto estimuló a la comunidad científica, 

Resulta lógico que el número de artículos científicos proponiendo modelos que permitiesen resolver los retos que presenta NIML aumentase, debido al comienzo de este tercer verano.


De los que se destacan los Modelos Ocultos de Markov, el Sparse Coding y finalmente las Redes Neuronales. Siendo estas últimas el foco principal del trabajo, y de esta sección. 



\subsubsection{Identificando Datos Ocultos Con Cadenas de Markov}
Una cadena de Markov resulta útil cuando se tiene que modelar un comportamiento en base a una secuencia de eventos observables, siendo L.E. Baum y Ted Petrie los autores de \autocite{introductorHMM}, el primero de una serie de papers sobre modelos probabilísticos y estadística que introdujeron los HMM.
Estos eventos observables son elementos conocidos, pero la secuencia de estados del modelo para esos eventos observables son desconocidos. 

El problema, como en NILM, es que los estados no son directamente observables. Estan \textit{ocultos} y por tanto debemos ajustar la cadena para incluir una secuencia de posibles similitudes observadas \autocite{markovStandford}. 

En el caso de una cadena de Markov. Dados una serie de eventos observables, podemos inferir la probabilidad de cada estado que causaron estos eventos observables: Calculando la probabilidad de una cadena de eventos observables dada la probabilidad de una cadena de estados del conjunto, para todas las posibles combinaciones de la cadena los estados del conjunto. 
Este algoritmo se conoce como el algoritmo de Viterbi. Kaustav Basu propone un modelo basado en este algoritmo. Sin embargo, el modelo HMM mostró menor rendimiento que el algoritmo KNN \autocite{Kaustav2015}.

Este es el modelado que permite inferir de manera probabilística.
Se destaca sobre las cadenas de markov una serie de algoritmos para determinar la cadena de estados ocultos más probable para la cadena de eventos observables, que se omiten por brevedad, pero pueden consultarse en \autocite{markovStandford}, para una detallada explicación sobre este área.

\subsubsection{Modelos Ocultos de Markov}
Explicadas brevemente las bases de los Modelos Ocultos de Markov,se detalla la aplicación de este modelo estadístico en la disgregación de consumos. 

Dado que un modelo de Markov oculto infiere probabilísticamente una serie de estados, esto nos resulta útil para la disgregación de dispositivos. Para codificar diferentes estados de diferentes dispositivos se aumenta el número de cadenas del modelo. Teniendo una matriz de estados $S_i^j$ donde $S$ es un conjunto de estados; las observaciones dependen de todos los estados en un momento dado.

Los Modelos de Markov Ocultos se han tratado en la literatura como efectivos en comparación con otras técnicas de monitorización cuando el entrenamiento supervisado presenta una baja tasa de muestreo en el eje temporal de la obtención de datos \autocite{frenchHMMNILM}.
Otras ventajas que presenta es su bajo coste computacional en el entrenamiento. Pero el uso de este tipo de elementos lo vuelven altamente susceptible a máximos locales\autocite{NILMreview2017}.

En el caso de los Modelos Factoriales de Markov Ocultos (con siglas FHMM en inglés), presentan ciertas ventajas en comparación con los HMM \footnote{Hidden Markov Models, o en español los Modelos Ocultos de Markov}:
La complejidad y el coste computacional de los los algoritmos de aprendizaje e inferencia son menores para los HMM. Sin embargo, tanto FHMM como HMM presentan limitaciones al volumen máximo de dispositivos simultáneos, incluso en casos como en la investigación de Kolter, J. Ziko \autocite{afmap2012} donde se propone un nuevo algoritmo no supervisado. El algoritmo desarrollado (AFMAP) no presenta los problemas de alta complejidad y máximos locales, pero requiere de etiquetación manual después de la disgregación. Y presenta bajo rendimiento para dispositivos electrónicos y de cocina \autocite[5]{NILMreview2017}.

El modelo propuesto por Parson et. al. 2014 \autocite{Parson2014} presenta un alto rendimiento para clasificar dispositivos con un número de estados finitos.


\section{Redes Neuronales}
\label{RN}

En esta sección se tratan tres conceptos de las Redes Neuronales; se dará una introducción y se expondrán las aplicaciones pertinentes al tema del proyecto.
\subsection{Sparse Coding}
Introducido por Olshausen et. al. 1997 \autocite{OLSHAUSEN}. El Sparse Coding consiste en reducir la activación de la capa oculta de las neuronas, forzando a la red neuronal a realizar un aprendizaje \autocite{stanfordSparse}.
Su objetivo es encontrar un conjunto de vectores $\phi_i$ tal que podamos representar un vector $x$ como una combinación lineal de estos vectores: 
$$
x = \sum_{i=1}^{k}a_i\phi_i
$$
Este es un concepto que está fuertemente relacionado con los autoencoders y el aprendizaje PCA. Que tienen como objetivo es que su salida sea igual a la entrada, pero limitando la capa oculta para forzar un aprendizaje \autocite{stanfordAutoEncoding}.
Por ejemplo, suponiendo que se buscase utilizar un autoencoder para comprimir una imagen para enviarla por internet. Si no se implementasen restricciones, dado que el objetivo de la red neuronal es reducir la pérdida de información a 0; la salida del autoencoder sería la imagen, completamente idéntica.

Para evitar esto se introduce una capa oculta de menor tamaño en comparación a la capa de entrada como restricción para forzar un aprendizaje, al tener una capa oculta, la capa de salida (de igual tamaño a la de entrada) debe \enquote{reconstruir} la entrada \autocite{stanfordAutoEncoding} .

Volviendo al Sparse Coding. Al tener un conjunto de vectores sobrecompleto, los coeficientes $a_i$ ya no se determinan únicamente por el vector de entrada $x$. Luego se define una función de escasez\footnote{De ahí el \enquote{Sparse}; escasez en Español}. Definimos esta función de escasez como: tener los menos componentes ($a_i$) distintos de cero o el mínimo de componentes no cerca de cero \autocite{stanfordSparse}.

Tanto los autoencoders como el sparse coding han sido usados en la literatura NILM.

En la solución propuesta por Mengheng Xue en \autocite{SparseCodingNILM}, buscan separar una señal de consumo agregada sin dar información sobre sus componentes, identificando componentes a través de una arquitectura escasa. 


\subsection{Redes Recurrentes}


Introducidas por una serie de investigadores: Rumelhart et. al. 1986 \autocite{Rumelhart1986LearningRB}; introdujo el algoritmo de retropropagación, una herramienta esencial en el entrenamiento de redes neuronales. Elman(1990) avanzó sobre las propuestas de Jordan(1986)\autocite{Jordan1986} creando modelos que integrasen representaciones temporales de datos \autocite{Elman1990} . Las redes recurrentes son un tipo de red neuronal profunda. Son particularmente efectivas para procesar secuencias de datos, reteniendo información sobre estados previos en su procesado de entradas. Esto es especialmente útil para tareas de clasificación de series temporales, como NILM.
Un ejemplo específico de la aplicación de RNN en NILM es su uso en modelos de secuencia a secuencia (seq2seq), donde los datos de entrada son secuencias de mediciones de energía y las salidas son secuencias de estados de los aparatos (encendido/apagado) o su consumo específico \autocite{Huber2014}\autocite{Kaibin2018}.



\section{Especificación del Estado del Arte}
\label{SOTA}
Estado del arte más relacionado con los objetivos y el enfoque del proyecto.
\subsection{Codificación de imágenes para NILM} 

En la literatura, la codificación de imágenes es un enfoque presente en varios proyectos. Los modelos propuestos por Shikha Singh et al 2017 \autocite{Singh2017} trabajan sobre imágenes codificadas con una transformada de ondícula de 2 niveles, esto permite extraer diferentes resoluciones de un rango temporal de consumo.

En cuanto al uso de codificación GAF, Ang Gao et. al. 2023 \autocite{GAO2023109443} es el único hasta donde llegan nuestros conocimientos que ha hecho uso de esta codificación en relación al enfoque del trabajo.

\subsection{CSPNet}     
CSPNet es una arquitectura de redes neuronales introducida por Chien-Yao Wang et. al. 2020 \autocite{CSPNetOG}. 
Al aplicarse esta arquitectura a clasificadores de imágenes como ResNeXt \autocite{xie2017aggregated} o DenseNet \autocite{huang2018densely} , optimizó su rendimiento. Reduciendo costes de computación, uso de memoria y además beneficiando la velocidad de inferencia y la precisión de estos modelos \autocite{CSPNetOG}.

Hasta donde llegan nuestros conocimientos, no se ha llegado a proponer el uso de un clasificador de imágenes con arquitectura CSPNet integrada para realizar disgregación de consumo eléctrico. 

Hay ciertas similitudes en la literatura \autocite{Radoslav2023}. 
\autocite{Xia2019}

En la publicación de Michele D'Inececco \autocite{Inecco2019}, se explora el aprendizaje transferido con varios datasets, entre ellos REFIT e UK-DALE aplicando una red neuronal convolucional de 5 capas, el aprendizaje a transferir siendo las señales específicas de los dispositivos eléctricos via aprendizaje seq2point.

\subsection{Validación de Modelos: NILMTK}  
NILMTK, publicado por Nipun Batra et. al. 2014 \autocite{Nipun2014}, presenta un toolkit de código abierto diseñado para facilitar la investigación y el desarrollo en el campo. Consta de interfaz de usuario, librerías, datasets estándar, algoritmos de disgregación y la flexibilidad y extensibilidad para integrar nuevos algoritmos y datasets. 
En 2019, publica NILMTK-contrib \autocite{Nipun2019}, una extensión comunitaria de NILMTK que busca acelerar la investigación en NILM mediante contribuciones comunitarias. 

\newpage
En lo que respecta al impacto, actualmente no ha habido grandes contribuciones al proyecto, el repositorio de nilmtk-contrib lleva tres años sin recibir ninguna actualización y sólo tres forks de 57 han tenido actividad el último año.
